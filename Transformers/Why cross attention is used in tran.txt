Why cross attention is used in transformers:
 Cross attention allows model to focus on different parts of input sequences while generating output sequence.

What does that mean?
 Basically in each timestamp the output of decoder depend on two things
 1: what is generated till now
 2: What information encoder has given to decoder.

The first one is known to decoder by multi head attention because it already find the contextual embeddings which has information of how words are connected to each other.

For the second dependency which is what has encoder given to decoder can be done through cross attention.
Here it is important to note that the encoder does not give raw word embeddings, instead it gives contextual embeddings. Encoder outputs are already context-aware representations. Cross-attention aligns decoder queries with these contextual vectors.

For example:
We have a sentence -> I love mango and it's translation -> Mujhe aam pasand hai

now the multi head attention will output the embeddings of "mujhe", "aam", "pasand hai" while encoder will also give the embedding vectors of "I", "like", "mango".

Now the second dependency is also done.

Now cross attention is mainly for finding the relation between these two different sequences i-e relation of I with mujhe,
relation of mango with aam and relation of like with pasand hai.

Now cross attention do it similarly just like self attention but the only difference is that here instead of generating all the Query, key and value embeddings from one single embedding it will generate the QUERY Vector from the Urdu word embedding i-e QUERY(mujhe) and the Key and value embedding will be generated from English sentence i-e KEY(I), VALUE(I).

In short Query will be generated by output sequence while key and value will be generated by input sequence.

So if we summarize this in a sentence.
"CROSS ATTENTION is for finding the relationship of output sequence from decoders multi head attention with the encoder input sequence (encoder output as input to decoder)."