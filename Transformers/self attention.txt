Lets suppose we have two sentences "Money bank grows" and "River Bank flows". Now by word embedding both the banks will have same vectors while both contexts are different. But self attention generate the vectors with some other method.
It take the same word embedding vector let's suppose for sentence one "bank" and will will generate 3 different vectors one for key, one for Query and one for Value. 
These 3 Vectors are generated by simple Linear transformation of the original word embedding vector of bank. So now we would have (keybank), (QueryBank) and (ValueBank) vectors and also we will have the same for 3 vectors for word MONEY and GROWS so in total we would have 9 Vectors. Now for generating new context Vector for the word "BANK" we will multiply the Query vector with the KEY vector of each word.
For example:
	(QueryBANK) * (KEYMONEY) then (QUERYBANK) * (KEYBANK) and then (QUERYBANK * KEYGROWS) 
by this we would have 3 different scalar values like S1, S2, S3 and we will scale them by the sqrt(dimensions of the key vector of word BANK). (Scaling prevents high variance of the dot product and prevent softmax from Vanishing gradient). Mathematically  (QBANK . KEYOF ALL words)/SQRT(dimensions of KEY). 
Then softmax activation function will be applied on all 3 scalar values to normalize the probablity distribution. And we will get 3 new scalar let's say W1, W2, W3 now these new scalars will be muliplied to VALUE Vectors of the each word i-e
(ValueMONEY), (VALUEBANK) and (VALUEGROWS) and finally all the three vectors will be added and we will have a new contextual and task specific embedding vector of the word BANK which will be different from the vector of word bank in the second sentence. 

Mathematical formula = Attention(Q,K,V)=softmax(QK(transpose)/SQRT(d(K))â€‹)V where Q is for Query, K is for KEY and V is for Value and dk is dimension of key vector of the word. 

Also the Values of the matrix through which we are doing linear transformation are parameters we would initialize it randomly and then it adjust itself while training.